{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44673590-17e6-4c2b-ac9e-c5b759971dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443c6a1c-69c9-490b-8c70-40ed9281d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProcessGrouping1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef503522-962d-4440-89cb-1e74026e26f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+--------+---------+\n",
      "|FromServer|ToServer|time|  action|processId|\n",
      "+----------+--------+----+--------+---------+\n",
      "|      null| lkVpiJ4|   0| Request|        1|\n",
      "|   lkVpiJ4|    null|   6|Response|        1|\n",
      "|      null| lkVpiJ4|   9| Request|        2|\n",
      "|   lkVpiJ4|    null|  12|Response|        2|\n",
      "|      null| OZBsEf0|  11| Request|        3|\n",
      "|   OZBsEf0|    null|  13|Response|        3|\n",
      "|      null|    Aum3|  18| Request|        4|\n",
      "|      Aum3|    null|  28|Response|        4|\n",
      "|      null|    Aum3|  22| Request|        5|\n",
      "|      Aum3|    null|  24|Response|        5|\n",
      "|      null|   qZGv1|  27| Request|        6|\n",
      "|     qZGv1|    null|  36|Response|        6|\n",
      "+----------+--------+----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data into a DataFrame\n",
    "data_path = \"logs2.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d27b1d-6b56-4f22-8885-67af12f535ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|processID|             actions|         actions_str|\n",
      "+---------+--------------------+--------------------+\n",
      "|        1|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|\n",
      "|        6|[{null, qZGv1, 27...|  nullqZGv1qZGv1null|\n",
      "|        3|[{null, OZBsEf0, ...|nullOZBsEf0OZBsEf...|\n",
      "|        5|[{null, Aum3, 22,...|    nullAum3Aum3null|\n",
      "|        4|[{null, Aum3, 18,...|    nullAum3Aum3null|\n",
      "|        2|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|\n",
      "+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, collect_list, struct\n",
    "\n",
    "# Group by processID and collect the sequence of actions\n",
    "processes_df = df.groupBy(\"processID\").agg(collect_list(struct(\"FromServer\", \"ToServer\", \"time\", \"action\")).alias(\"actions\"))\n",
    "\n",
    "# Convert actions to string for MinHash LSH\n",
    "def actions_to_string(actions):\n",
    "    return \"\".join([f\"{action['FromServer']}{action['ToServer']}\" for action in actions])\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "actions_to_string_udf = udf(actions_to_string, StringType())\n",
    "processes_df = processes_df.withColumn(\"actions_str\", actions_to_string_udf(col(\"actions\")))\n",
    "processes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8079d96a-2044-4c38-a001-4c14c5c4af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|processID|             actions|         actions_str|            shingles|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|        1|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|\n",
      "|        6|[{null, qZGv1, 27...|  nullqZGv1qZGv1null|[nullq, ullqZ, ll...|\n",
      "|        3|[{null, OZBsEf0, ...|nullOZBsEf0OZBsEf...|[nullO, ullOZ, ll...|\n",
      "|        5|[{null, Aum3, 22,...|    nullAum3Aum3null|[nullA, ullAu, ll...|\n",
      "|        4|[{null, Aum3, 18,...|    nullAum3Aum3null|[nullA, ullAu, ll...|\n",
      "|        2|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert actions string into shingles\n",
    "def get_shingles(row, k=5):\n",
    "    concatenated_str = ''.join(row)\n",
    "    shingles = [concatenated_str[i:i+k] for i in range(len(concatenated_str) - (k - 1))]\n",
    "    return shingles\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "get_shingles_udf = udf(lambda x: get_shingles(x), ArrayType(StringType()))\n",
    "processes_df = processes_df.withColumn(\"shingles\", get_shingles_udf(col(\"actions_str\")))\n",
    "processes_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47ab9690-71b3-467c-8a3e-9a5d0977297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|processID|             actions|         actions_str|            shingles|            features|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        1|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|(55,[0,1,2,6,9,13...|\n",
      "|        6|[{null, qZGv1, 27...|  nullqZGv1qZGv1null|[nullq, ullqZ, ll...|(55,[4,31,33,37,3...|\n",
      "|        3|[{null, OZBsEf0, ...|nullOZBsEf0OZBsEf...|[nullO, ullOZ, ll...|(55,[8,21,26,32,3...|\n",
      "|        5|[{null, Aum3, 22,...|    nullAum3Aum3null|[nullA, ullAu, ll...|(55,[3,5,7,10,11,...|\n",
      "|        4|[{null, Aum3, 18,...|    nullAum3Aum3null|[nullA, ullAu, ll...|(55,[3,5,7,10,11,...|\n",
      "|        2|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|(55,[0,1,2,6,9,13...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "None\n",
      "Distinct Attributes (Vocabulary):\n",
      "0. lkVpi\n",
      "1. VpiJ4\n",
      "2. kVpiJ\n",
      "3. um3nu\n",
      "4. qZGv1\n",
      "5. m3nul\n",
      "6. piJ4l\n",
      "7. ullAu\n",
      "8. ZBsEf\n",
      "9. lllkV\n",
      "10. 3Aum3\n",
      "11. m3Aum\n",
      "12. 3null\n",
      "13. llkVp\n",
      "14. nullA\n",
      "15. 4lkVp\n",
      "16. Aum3n\n",
      "17. um3Au\n",
      "18. iJ4lk\n",
      "19. lAum3\n",
      "20. 4null\n",
      "21. OZBsE\n",
      "22. J4lkV\n",
      "23. ulllk\n",
      "24. llAum\n",
      "25. J4nul\n",
      "26. BsEf0\n",
      "27. piJ4n\n",
      "28. nulll\n",
      "29. Aum3A\n",
      "30. iJ4nu\n",
      "31. Gv1nu\n",
      "32. llOZB\n",
      "33. Gv1qZ\n",
      "34. sEf0O\n",
      "35. Ef0nu\n",
      "36. Ef0OZ\n",
      "37. llqZG\n",
      "38. v1qZG\n",
      "39. 1qZGv\n",
      "40. nullq\n",
      "41. ullqZ\n",
      "42. ullOZ\n",
      "43. 0OZBs\n",
      "44. 1null\n",
      "45. f0OZB\n",
      "46. 0null\n",
      "47. ZGv1n\n",
      "48. sEf0n\n",
      "49. f0nul\n",
      "50. lqZGv\n",
      "51. lOZBs\n",
      "52. v1nul\n",
      "53. ZGv1q\n",
      "54. nullO\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"shingles\", outputCol=\"features\",binary=True)\n",
    "cv_model = cv.fit(processes_df)\n",
    "vectorized_df = cv_model.transform(processes_df)\n",
    "print(vectorized_df.show(truncate=True))\n",
    "\n",
    "print(\"Distinct Attributes (Vocabulary):\")\n",
    "for i, attr in enumerate(cv_model.vocabulary):\n",
    "    print(f\"{i}. {attr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca2f195-e075-428f-a4ea-ebb0cdd7705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10)\n",
    "mh_model = mh.fit(vectorized_df)\n",
    "hashed_df = mh_model.transform(vectorized_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a097b3c4-b0b0-40c8-8190-2f43a2d3511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------------+\n",
      "|processID_A|processID_B|JaccardDistance|\n",
      "+-----------+-----------+---------------+\n",
      "|          4|          5|            0.0|\n",
      "|          1|          2|            0.0|\n",
      "+-----------+-----------+---------------+\n",
      "\n",
      "processes_df\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|processID|             actions|         actions_str|            shingles|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|        1|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|\n",
      "|        6|[{null, qZGv1, 27...|  nullqZGv1qZGv1null|[nullq, ullqZ, ll...|\n",
      "|        3|[{null, OZBsEf0, ...|nullOZBsEf0OZBsEf...|[nullO, ullOZ, ll...|\n",
      "|        5|[{null, Aum3, 22,...|    nullAum3Aum3null|[nullA, ullAu, ll...|\n",
      "|        4|[{null, Aum3, 18,...|    nullAum3Aum3null|[nullA, ullAu, ll...|\n",
      "|        2|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "\n",
      "similarity_df\n",
      "+-----------+-----------+---------------+\n",
      "|processID_A|processID_B|JaccardDistance|\n",
      "+-----------+-----------+---------------+\n",
      "|          4|          5|            0.0|\n",
      "|          1|          2|            0.0|\n",
      "+-----------+-----------+---------------+\n",
      "\n",
      "+-----------+-----------+---------------+--------------------+--------------------+-----------------+\n",
      "|processID_A|processID_B|JaccardDistance|          shingles_A|          shingles_B|JaccardSimilarity|\n",
      "+-----------+-----------+---------------+--------------------+--------------------+-----------------+\n",
      "|          4|          5|            0.0|[nullA, ullAu, ll...|[nullA, ullAu, ll...|              1.0|\n",
      "|          1|          2|            0.0|[nulll, ulllk, ll...|[nulll, ulllk, ll...|              1.0|\n",
      "+-----------+-----------+---------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, array_union, collect_list, explode, col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "threshold = 0.8\n",
    "# Find similar candidate process IDs using MinHashLSH\n",
    "similarity_df = mh_model.approxSimilarityJoin(hashed_df, hashed_df, threshold, distCol=\"JaccardDistance\") \\\n",
    "    .select(col(\"datasetA.processID\").alias(\"processID_A\"),\n",
    "            col(\"datasetB.processID\").alias(\"processID_B\"),\n",
    "            col(\"JaccardDistance\"))\n",
    "\n",
    "# Filter out self-joins and duplicates\n",
    "similarity_df = similarity_df.filter(col(\"processID_A\") < col(\"processID_B\"))\n",
    "similarity_df.show()\n",
    "# Function to calculate Jaccard similarity\n",
    "def jaccard_similarity(x, y):\n",
    "    x_set = set(x)\n",
    "    y_set = set(y)\n",
    "    intersection = len(x_set & y_set)\n",
    "    union = len(x_set | y_set)\n",
    "    return float(intersection) / union\n",
    "\n",
    "# Register the function as a UDF\n",
    "from pyspark.sql.types import FloatType\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "print(\"processes_df\")\n",
    "processes_df.show()\n",
    "print(\"similarity_df\")\n",
    "similarity_df.show()\n",
    "\n",
    "# Join with original DataFrame to get shingles for each process\n",
    "similarity_df = similarity_df \\\n",
    "    .join(processes_df.select(\"processID\", \"shingles\"), similarity_df.processID_A == processes_df.processID) \\\n",
    "    .withColumnRenamed(\"shingles\", \"shingles_A\") \\\n",
    "    .drop(\"processID\") \\\n",
    "    .join(processes_df.select(\"processID\", \"shingles\"), similarity_df.processID_B == processes_df.processID) \\\n",
    "    .withColumnRenamed(\"shingles\", \"shingles_B\") \\\n",
    "    .drop(\"processID\")\n",
    "\n",
    "# Calculate Jaccard similarity for each candidate pair\n",
    "similarity_df = similarity_df.withColumn(\"JaccardSimilarity\", jaccard_similarity_udf(col(\"shingles_A\"), col(\"shingles_B\")))\n",
    "\n",
    "# Filter pairs with Jaccard similarity above a threshold (e.g., 0.8)\n",
    "similarity_df = similarity_df.filter(col(\"JaccardSimilarity\") >= 1- threshold)\n",
    "\n",
    "# Group by processID_A and collect similar processIDs\n",
    "grouped_df = similarity_df.groupBy(\"processID_A\").agg(collect_list(\"processID_B\").alias(\"similar_processIDs\"))\n",
    "\n",
    "# Convert processID_A to an array and concatenate with similar_processIDs\n",
    "grouped_df = grouped_df.withColumn(\"all_processIDs\", array_union(array(col(\"processID_A\")), col(\"similar_processIDs\")))\n",
    "\n",
    "# Explode the all_processIDs array to get a mapping of each process ID to its group\n",
    "exploded_df = grouped_df.select(explode(col(\"all_processIDs\")).alias(\"processID\"), col(\"processID_A\").alias(\"group_representative\"))\n",
    "similarity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5dcfb7e-4d37-4ddc-a840-b731a251c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+--------+\n",
      "|processID|FromServer|ToServer|time|  action|\n",
      "+---------+----------+--------+----+--------+\n",
      "|        1|      null| lkVpiJ4|   0| Request|\n",
      "|        1|   lkVpiJ4|    null|   6|Response|\n",
      "|        4|      null|    Aum3|  18| Request|\n",
      "|        4|      Aum3|    null|  28|Response|\n",
      "+---------+----------+--------+----+--------+\n",
      "\n",
      "+-----+-----------+\n",
      "|Group|final_group|\n",
      "+-----+-----------+\n",
      "|  1_2|     [1, 2]|\n",
      "|  4_5|     [4, 5]|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge overlapping groups\n",
    "def merge_groups(group_list):\n",
    "    groups = []\n",
    "    for group in group_list:\n",
    "        merged = False\n",
    "        for existing_group in groups:\n",
    "            if any(item in group for item in existing_group):\n",
    "                existing_group.update(group)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            groups.append(set(group))\n",
    "    return [list(group) for group in groups]\n",
    "\n",
    "merge_groups_udf = udf(lambda x: merge_groups(x), ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "grouped_lists = exploded_df.groupBy(\"group_representative\") \\\n",
    "    .agg(collect_list(\"processID\").alias(\"group_list\")) \\\n",
    "    .agg(collect_list(\"group_list\").alias(\"group_lists\"))\n",
    "\n",
    "merged_groups = grouped_lists.withColumn(\"merged_groups\", merge_groups_udf(col(\"group_lists\"))) \\\n",
    "    .select(explode(col(\"merged_groups\")).alias(\"final_group\"))\n",
    "\n",
    "# Convert the final groups to a DataFrame\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "final_groups_df = merged_groups.select(concat_ws(\"_\", col(\"final_group\")).alias(\"Group\"), col(\"final_group\"))\n",
    "\n",
    "# Find the representative process for each final group\n",
    "final_groups_exploded = final_groups_df.withColumn(\"processID\", explode(col(\"final_group\")))\n",
    "\n",
    "# Join with the original DataFrame to keep only the representative process\n",
    "filtered_df = df.join(final_groups_exploded, on=\"processID\", how=\"inner\")\n",
    "\n",
    "# Select the smallest processID in each group as the representative\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "group_representative_df = final_groups_exploded.groupBy(\"Group\").agg(min(\"processID\").alias(\"representative_processID\"))\n",
    "\n",
    "# Join to get the full details of the representative processes\n",
    "representative_processes_df = group_representative_df.join(filtered_df, filtered_df[\"processID\"] == group_representative_df.representative_processID, \"inner\") \\\n",
    "    .select(\"processID\", \"FromServer\", \"ToServer\", \"time\", \"action\")\n",
    "representative_processes_df.show()\n",
    "final_groups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b518d564-5cfe-4bb7-b8c7-6af2c0d4bc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+--------+\n",
      "|processID|FromServer|ToServer|time|action  |\n",
      "+---------+----------+--------+----+--------+\n",
      "|1        |null      |lkVpiJ4 |0   |Request |\n",
      "|1        |lkVpiJ4   |null    |6   |Response|\n",
      "|4        |null      |Aum3    |18  |Request |\n",
      "|4        |Aum3      |null    |28  |Response|\n",
      "+---------+----------+--------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the final result\n",
    "representative_processes_df.show(truncate=False)\n",
    "\n",
    "# Optional: Write the final groups to a file\n",
    "# final_groups_df.write.csv(\"path_to_output_groups_file.csv\", header=True)\n",
    "# representative_processes_df.write.csv(\"path_to_output_filtered_file.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b5802c-cd0d-4286-b8fb-2eebdb8403b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+--------+\n",
      "|processId|FromServer|ToServer|time|action  |\n",
      "+---------+----------+--------+----+--------+\n",
      "|3        |null      |OZBsEf0 |11  |Request |\n",
      "|3        |OZBsEf0   |null    |13  |Response|\n",
      "|6        |null      |qZGv1   |27  |Request |\n",
      "|6        |qZGv1     |null    |36  |Response|\n",
      "|7        |null      |lkVpiJ4 |0   |Request |\n",
      "|7        |lkVpiJ4   |null    |6   |Response|\n",
      "|10       |null      |Aum3    |18  |Request |\n",
      "|10       |Aum3      |null    |28  |Response|\n",
      "+---------+----------+--------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "# Step 1: Remove Processes in Groups\n",
    "# Get the list of process IDs to remove\n",
    "processes_to_remove = final_groups_df.selectExpr(\"explode(final_group) as processID\").distinct()\n",
    "\n",
    "# Filter out rows where processID is in processes_to_remove\n",
    "df_without_groups = df.join(processes_to_remove, \"processID\", \"left_anti\")\n",
    "\n",
    "# Add a constant number to processID\n",
    "constant_number = df.agg({\"processID\": \"max\"}).first()[0]\n",
    "new_representative_processes_df = representative_processes_df.withColumn(\n",
    "    \"processID\",\n",
    "    expr(f\"processID + {constant_number}\")\n",
    ")\n",
    "\n",
    "# Show the final DataFrame\n",
    "new_representative_processes_df.select(\"processID\", \"FromServer\", \"ToServer\", \"time\", \"action\").orderBy(\"time\")\n",
    "\n",
    "# Combine original DataFrame and representatives DataFrame\n",
    "combined_df = df_without_groups.union(new_representative_processes_df)\n",
    "\n",
    "# Show final combined DataFrame\n",
    "combined_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3cf700-0abb-438d-b73c-1ac8a303d7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
