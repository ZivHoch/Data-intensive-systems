{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44673590-17e6-4c2b-ac9e-c5b759971dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import concat, lit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824c81f-680a-419a-93a9-fc8274862528",
   "metadata": {},
   "source": [
    "# code to start the Master:\n",
    "1. Open cmd and admin\n",
    "2. write \"cd %SPARK_HOME%\"\n",
    "3. bin\\spark-class2.cmd org.apache.spark.deploy.master.Master\n",
    "# code to start the worker:\n",
    "1. Open cmd and admin\n",
    "2. write \"cd %SPARK_HOME%\"\n",
    "3. write \"bin\\spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 6 -m 10G spark://192.168.1.81:7077\"\n",
    "* in step 3:\n",
    "* -c -> number of cores\n",
    "* -m -> amount of RAM for the current worker\n",
    "* the spark link is from the Master link ( go to the web page of the master and locate the spark link )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443c6a1c-69c9-490b-8c70-40ed9281d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"part1Grouping\") \\\n",
    "    .master(\"spark://192.168.1.81:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.cores\", \"6\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.cores\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"3600s\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef503522-962d-4440-89cb-1e74026e26f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+--------+---------+\n",
      "|FromServer|ToServer|time|  action|processId|\n",
      "+----------+--------+----+--------+---------+\n",
      "|      null| lkVpiJ4|   0| Request|        1|\n",
      "|   lkVpiJ4|    null|   6|Response|        1|\n",
      "|      null| lkVpiJ4|   9| Request|        2|\n",
      "|   lkVpiJ4|    null|  12|Response|        2|\n",
      "|      null| OZBsEf0|  11| Request|        3|\n",
      "|   OZBsEf0|    null|  13|Response|        3|\n",
      "|      null|    Aum3|  18| Request|        4|\n",
      "|      Aum3|    null|  28|Response|        4|\n",
      "|      null|    Aum3|  22| Request|        5|\n",
      "|      Aum3|    null|  24|Response|        5|\n",
      "|      null|   qZGv1|  27| Request|        6|\n",
      "|     qZGv1|    null|  36|Response|        6|\n",
      "|      null|    asdf|  40| Request|        7|\n",
      "|      asdf|    fdsa|  41| Request|        7|\n",
      "|      fdsa|    asdf|  42|Response|        7|\n",
      "|      asdf|    null|  43|Response|        7|\n",
      "+----------+--------+----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data into a DataFrame\n",
    "data_path = \"output2.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d27b1d-6b56-4f22-8885-67af12f535ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|processID|             actions|         actions_str|\n",
      "+---------+--------------------+--------------------+\n",
      "|        1|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|\n",
      "|        6|[{null, qZGv1, 27...|  nullqZGv1qZGv1null|\n",
      "|        3|[{null, OZBsEf0, ...|nullOZBsEf0OZBsEf...|\n",
      "|        5|[{null, Aum3, 22,...|    nullAum3Aum3null|\n",
      "|        4|[{null, Aum3, 18,...|    nullAum3Aum3null|\n",
      "|        7|[{null, asdf, 40,...|nullasdfasdffdsaf...|\n",
      "|        2|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|\n",
      "+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, collect_list, struct\n",
    "\n",
    "# Group by processID and collect the sequence of actions\n",
    "processes_df = df.groupBy(\"processID\").agg(collect_list(struct(\"FromServer\", \"ToServer\", \"time\", \"action\")).alias(\"actions\"))\n",
    "\n",
    "# Convert actions to string for MinHash LSH\n",
    "def actions_to_string(actions):\n",
    "    return \"\".join([f\"{action['FromServer']}{action['ToServer']}\" for action in actions])\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "actions_to_string_udf = udf(actions_to_string, StringType())\n",
    "processes_df = processes_df.withColumn(\"actions_str\", actions_to_string_udf(col(\"actions\")))\n",
    "processes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8079d96a-2044-4c38-a001-4c14c5c4af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|processID|             actions|         actions_str|            shingles|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|        1|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|\n",
      "|        6|[{null, qZGv1, 27...|  nullqZGv1qZGv1null|[nullq, ullqZ, ll...|\n",
      "|        3|[{null, OZBsEf0, ...|nullOZBsEf0OZBsEf...|[nullO, ullOZ, ll...|\n",
      "|        5|[{null, Aum3, 22,...|    nullAum3Aum3null|[nullA, ullAu, ll...|\n",
      "|        4|[{null, Aum3, 18,...|    nullAum3Aum3null|[nullA, ullAu, ll...|\n",
      "|        7|[{null, asdf, 40,...|nullasdfasdffdsaf...|[nulla, ullas, ll...|\n",
      "|        2|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert actions string into shingles\n",
    "def get_shingles(row, k=5):\n",
    "    concatenated_str = ''.join(row)\n",
    "    shingles = [concatenated_str[i:i+k] for i in range(len(concatenated_str) - (k - 1))]\n",
    "    return shingles\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "get_shingles_udf = udf(lambda x: get_shingles(x), ArrayType(StringType()))\n",
    "processes_df = processes_df.withColumn(\"shingles\", get_shingles_udf(col(\"actions_str\")))\n",
    "processes_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47ab9690-71b3-467c-8a3e-9a5d0977297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|processID|             actions|         actions_str|            shingles|            features|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        1|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|(79,[0,1,2,6,10,1...|\n",
      "|        6|[{null, qZGv1, 27...|  nullqZGv1qZGv1null|[nullq, ullqZ, ll...|(79,[4,37,40,47,4...|\n",
      "|        3|[{null, OZBsEf0, ...|nullOZBsEf0OZBsEf...|[nullO, ullOZ, ll...|(79,[9,25,30,38,4...|\n",
      "|        5|[{null, Aum3, 22,...|    nullAum3Aum3null|[nullA, ullAu, ll...|(79,[3,5,8,11,13,...|\n",
      "|        4|[{null, Aum3, 18,...|    nullAum3Aum3null|[nullA, ullAu, ll...|(79,[3,5,8,11,13,...|\n",
      "|        7|[{null, asdf, 40,...|nullasdfasdffdsaf...|[nulla, ullas, ll...|(79,[7,12,15,21,3...|\n",
      "|        2|[{null, lkVpiJ4, ...|nulllkVpiJ4lkVpiJ...|[nulll, ulllk, ll...|(79,[0,1,2,6,10,1...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"shingles\", outputCol=\"features\",binary=True)\n",
    "cv_model = cv.fit(processes_df)\n",
    "vectorized_df = cv_model.transform(processes_df)\n",
    "print(vectorized_df.show(truncate=True))\n",
    "\n",
    "# print(\"Distinct Attributes (Vocabulary):\")\n",
    "# for i, attr in enumerate(cv_model.vocabulary):\n",
    "#     print(f\"{i}. {attr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca2f195-e075-428f-a4ea-ebb0cdd7705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10)\n",
    "mh_model = mh.fit(vectorized_df)\n",
    "hashed_df = mh_model.transform(vectorized_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a097b3c4-b0b0-40c8-8190-2f43a2d3511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------------+\n",
      "|processID_A|processID_B|JaccardDistance|\n",
      "+-----------+-----------+---------------+\n",
      "|          4|          5|            0.0|\n",
      "|          1|          2|            0.0|\n",
      "+-----------+-----------+---------------+\n",
      "\n",
      "+-----------+-----------+---------------+--------------------+--------------------+-----------------+\n",
      "|processID_A|processID_B|JaccardDistance|          shingles_A|          shingles_B|JaccardSimilarity|\n",
      "+-----------+-----------+---------------+--------------------+--------------------+-----------------+\n",
      "|          4|          5|            0.0|[nullA, ullAu, ll...|[nullA, ullAu, ll...|              1.0|\n",
      "|          1|          2|            0.0|[nulll, ulllk, ll...|[nulll, ulllk, ll...|              1.0|\n",
      "+-----------+-----------+---------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, array_union, collect_list, explode, col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "threshold = 0.8\n",
    "# Find similar candidate process IDs using MinHashLSH\n",
    "similarity_df = mh_model.approxSimilarityJoin(hashed_df, hashed_df, threshold, distCol=\"JaccardDistance\") \\\n",
    "    .select(col(\"datasetA.processID\").alias(\"processID_A\"),\n",
    "            col(\"datasetB.processID\").alias(\"processID_B\"),\n",
    "            col(\"JaccardDistance\"))\n",
    "\n",
    "# Filter out self-joins and duplicates\n",
    "similarity_df = similarity_df.filter(col(\"processID_A\") < col(\"processID_B\"))\n",
    "similarity_df.show()\n",
    "# Function to calculate Jaccard similarity\n",
    "def jaccard_similarity(x, y):\n",
    "    x_set = set(x)\n",
    "    y_set = set(y)\n",
    "    intersection = len(x_set & y_set)\n",
    "    union = len(x_set | y_set)\n",
    "    return float(intersection) / union\n",
    "\n",
    "# Register the function as a UDF\n",
    "from pyspark.sql.types import FloatType\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "# print(\"processes_df\")\n",
    "# processes_df.show()\n",
    "# print(\"similarity_df\")\n",
    "# similarity_df.show()\n",
    "\n",
    "# Join with original DataFrame to get shingles for each process\n",
    "similarity_df = similarity_df \\\n",
    "    .join(processes_df.select(\"processID\", \"shingles\"), similarity_df.processID_A == processes_df.processID) \\\n",
    "    .withColumnRenamed(\"shingles\", \"shingles_A\") \\\n",
    "    .drop(\"processID\") \\\n",
    "    .join(processes_df.select(\"processID\", \"shingles\"), similarity_df.processID_B == processes_df.processID) \\\n",
    "    .withColumnRenamed(\"shingles\", \"shingles_B\") \\\n",
    "    .drop(\"processID\")\n",
    "\n",
    "# Calculate Jaccard similarity for each candidate pair\n",
    "similarity_df = similarity_df.withColumn(\"JaccardSimilarity\", jaccard_similarity_udf(col(\"shingles_A\"), col(\"shingles_B\")))\n",
    "# Filter pairs with Jaccard similarity above a threshold (e.g., 0.8)\n",
    "similarity_df_filtered = similarity_df.filter(col(\"JaccardSimilarity\") >= 0.9)\n",
    "# Group by processID_A and collect similar processIDs\n",
    "grouped_df = similarity_df.groupBy(\"processID_A\").agg(collect_list(\"processID_B\").alias(\"similar_processIDs\"))\n",
    "\n",
    "# Convert processID_A to an array and concatenate with similar_processIDs\n",
    "grouped_df = grouped_df.withColumn(\"all_processIDs\", array_union(array(col(\"processID_A\")), col(\"similar_processIDs\")))\n",
    "\n",
    "# Explode the all_processIDs array to get a mapping of each process ID to its group\n",
    "exploded_df = grouped_df.select(explode(col(\"all_processIDs\")).alias(\"processID\"), col(\"processID_A\").alias(\"group_representative\"))\n",
    "similarity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dcfb7e-4d37-4ddc-a840-b731a251c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+--------+\n",
      "|processID|FromServer|ToServer|time|  action|\n",
      "+---------+----------+--------+----+--------+\n",
      "|        1|      null| lkVpiJ4|   0| Request|\n",
      "|        1|   lkVpiJ4|    null|   6|Response|\n",
      "|        4|      null|    Aum3|  18| Request|\n",
      "|        4|      Aum3|    null|  28|Response|\n",
      "+---------+----------+--------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge overlapping groups\n",
    "def merge_groups(group_list):\n",
    "    groups = []\n",
    "    for group in group_list:\n",
    "        merged = False\n",
    "        for existing_group in groups:\n",
    "            if any(item in group for item in existing_group):\n",
    "                existing_group.update(group)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            groups.append(set(group))\n",
    "    return [list(group) for group in groups]\n",
    "\n",
    "merge_groups_udf = udf(lambda x: merge_groups(x), ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "grouped_lists = exploded_df.groupBy(\"group_representative\") \\\n",
    "    .agg(collect_list(\"processID\").alias(\"group_list\")) \\\n",
    "    .agg(collect_list(\"group_list\").alias(\"group_lists\"))\n",
    "\n",
    "merged_groups = grouped_lists.withColumn(\"merged_groups\", merge_groups_udf(col(\"group_lists\"))) \\\n",
    "    .select(explode(col(\"merged_groups\")).alias(\"final_group\"))\n",
    "\n",
    "# Convert the final groups to a DataFrame\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "final_groups_df = merged_groups.select(concat_ws(\"_\", col(\"final_group\")).alias(\"Group\"), col(\"final_group\"))\n",
    "\n",
    "# Find the representative process for each final group\n",
    "final_groups_exploded = final_groups_df.withColumn(\"processID\", explode(col(\"final_group\")))\n",
    "\n",
    "# Join with the original DataFrame to keep only the representative process\n",
    "filtered_df = df.join(final_groups_exploded, on=\"processID\", how=\"inner\")\n",
    "\n",
    "# Select the smallest processID in each group as the representative\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "group_representative_df = final_groups_exploded.groupBy(\"Group\").agg(min(\"processID\").alias(\"representative_processID\"))\n",
    "\n",
    "# Join to get the full details of the representative processes\n",
    "representative_processes_df = group_representative_df.join(filtered_df, filtered_df[\"processID\"] == group_representative_df.representative_processID, \"inner\") \\\n",
    "    .select(\"processID\", \"FromServer\", \"ToServer\", \"time\", \"action\")\n",
    "representative_processes_df.show()\n",
    "final_groups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518d564-5cfe-4bb7-b8c7-6af2c0d4bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the final result\n",
    "representative_processes_df.show(truncate=False)\n",
    "\n",
    "# Optional: Write the final groups to a file\n",
    "# final_groups_df.write.csv(\"path_to_output_groups_file.csv\", header=True)\n",
    "# representative_processes_df.write.csv(\"path_to_output_filtered_file.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5802c-cd0d-4286-b8fb-2eebdb8403b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "# Step 1: Remove Processes in Groups\n",
    "# Get the list of process IDs to remove\n",
    "processes_to_remove = final_groups_df.selectExpr(\"explode(final_group) as processID\").distinct()\n",
    "\n",
    "# Filter out rows where processID is in processes_to_remove\n",
    "df_without_groups = df.join(processes_to_remove, \"processID\", \"left_anti\")\n",
    "df_without_groups = df_without_groups.select(\"FromServer\", \"ToServer\", \"time\", \"action\",\"processID\")\n",
    "# Add a constant number to processID\n",
    "constant_number = df.agg({\"processID\": \"max\"}).first()[0]\n",
    "new_representative_processes_df = representative_processes_df.withColumn(\n",
    "    \"processID\",\n",
    "    expr(f\"processID + {constant_number}\")\n",
    ")\n",
    "\n",
    "# Show the final DataFrame\n",
    "new_representative_processes_df = new_representative_processes_df.select(\"FromServer\", \"ToServer\", \"time\", \"action\",\"processID\").orderBy(\"time\")\n",
    "\n",
    "# Combine original DataFrame and representatives DataFrame\n",
    "combined_df = df_without_groups.union(new_representative_processes_df)\n",
    "\n",
    "# Show final combined DataFrame\n",
    "combined_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fd6b4-fc29-4be1-bfe0-1fca39b52f08",
   "metadata": {},
   "source": [
    "# creating the txt files:\n",
    "## The desired files will be in the folder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf60b6-3e4f-4f7f-98ee-0a6a078a2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_one_txt(df, local_path_name,wanted_list):\n",
    "    correct_path = wanted_list + \"/part1Output.txt\"\n",
    "    formatted_df = df.withColumn(\n",
    "    \"formatted_line\",\n",
    "    concat(lit(\"<\"), df.FromServer, lit(\",\"),\n",
    "           df.ToServer, lit(\",\"),\n",
    "           df.time, lit(\",\"),\n",
    "           df.action, lit(\",\"),\n",
    "           df.processID, lit(\">\"))\n",
    ")\n",
    "    open(correct_path, \"w\")\n",
    "    formatted_df.select(\"formatted_line\").write.mode(\"overwrite\").text(output_path)\n",
    "    os.system(f'cat {local_path_name}/*.txt >> {correct_path}')\n",
    "    os.system(f'rm -r {local_path_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9f5fb-c15b-4912-9cbe-951cb816d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the output path\n",
    "output_path = \"./part1OUT1\"\n",
    "output_path1 = \"./output\"\n",
    "write_to_one_txt(combined_df,output_path,output_path1)\n",
    "# Write the DataFrame to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb262e7-90dd-43b2-a7c0-c429b39d384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe only with the processes that were grouped.\n",
    "df_with_groups = df.join(processes_to_remove, \"processID\", \"semi\")\n",
    "df_with_groups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ff333-dfeb-45fb-81fa-ae4feb700f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_final_groups_df = final_groups_df.select(\"Group\", explode(\"final_group\").alias(\"processID\"))\n",
    "exploded_final_groups_df.show()\n",
    "df_with_groups.show()\n",
    "joined_df = df_with_groups.join(exploded_final_groups_df, \"processID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a4565-628c-4fe5-9798-795445364f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format each row into the desired format\n",
    "formatted_df = joined_df.withColumn(\n",
    "    \"formatted_row\",\n",
    "    concat_ws(\"\", lit(\"<\"), col(\"FromServer\"), lit(\",\"), col(\"ToServer\"),\n",
    "              lit(\",\"), col(\"time\"), lit(\",\"), col(\"action\"), lit(\",\"), col(\"processID\"), lit(\">\"))\n",
    ")\n",
    "\n",
    "# Group by Group name and aggregate process IDs and formatted rows\n",
    "grouped_df = formatted_df.groupBy(\"Group\").agg(\n",
    "    collect_list(\"processID\").alias(\"processIDs\"),\n",
    "    collect_list(\"formatted_row\").alias(\"formatted_rows\")\n",
    ")\n",
    "\n",
    "# Function to write groups to txt file\n",
    "def write_groups_to_txt(grouped_df, output_path):\n",
    "    with open(output_path, \"w\") as file:\n",
    "        for row in grouped_df.collect():\n",
    "            group_name = row[\"Group\"]\n",
    "            process_ids = row[\"processIDs\"]\n",
    "            formatted_rows = row[\"formatted_rows\"]\n",
    "            \n",
    "            # Ensure process_ids are unique and sorted\n",
    "            process_ids = sorted(set(process_ids))\n",
    "            \n",
    "            file.write(f\"Group: {{{', '.join(map(str, process_ids))}}}\\n\")\n",
    "            \n",
    "            for process_id in process_ids:\n",
    "                file.write(f\"{process_id}:\\n\")\n",
    "                \n",
    "                # Find all formatted rows for the current process ID\n",
    "                rows_for_process_id = [row for row in formatted_rows if row.endswith(f\",{process_id}>\")]\n",
    "                \n",
    "                if rows_for_process_id:\n",
    "                    for formatted_row in rows_for_process_id:\n",
    "                        file.write(f\"{formatted_row}\\n\")\n",
    "                else:\n",
    "                    file.write(\"<No corresponding formatted rows found>\\n\")\n",
    "                    \n",
    "            file.write(\"\\n\")  # Add empty line between groups for clarity\n",
    "\n",
    "# Output path\n",
    "output_path = \"./output/part1Observations.txt\"\n",
    "\n",
    "# Call function to write to text file\n",
    "write_groups_to_txt(grouped_df, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa957dcb-e50c-48de-b574-f9b64135edc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
