{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44673590-17e6-4c2b-ac9e-c5b759971dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import concat, lit\n",
    "from pyspark.sql.functions import avg, length\n",
    "from pyspark.sql.functions import col, expr, count , row_number\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dab37d-1d4a-4b16-9a44-571c84826386",
   "metadata": {},
   "source": [
    "# PART 1:\n",
    "## 1. Grouping the similar processes according to Jaccard Similarities\n",
    "## 2. Creating the new data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824c81f-680a-419a-93a9-fc8274862528",
   "metadata": {},
   "source": [
    "# code to start the Master:\n",
    "1. Open cmd and admin\n",
    "2. write \"cd %SPARK_HOME%\"\n",
    "3. bin\\spark-class2.cmd org.apache.spark.deploy.master.Master\n",
    "# code to start the worker:\n",
    "1. Open cmd and admin\n",
    "2. write \"cd %SPARK_HOME%\"\n",
    "3. write \"bin\\spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 6 -m 10G spark://192.168.1.81:7077\"\n",
    "* in step 3:\n",
    "* -c -> number of cores\n",
    "* -m -> amount of RAM for the current worker\n",
    "* the spark link is from the Master link ( go to the web page of the master and locate the spark link )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443c6a1c-69c9-490b-8c70-40ed9281d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"part1Grouping\") \\\n",
    "    .master(\"spark://192.168.1.81:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"12\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.cores\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"3600s\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe5e88-af08-42d0-941c-97810d03995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_file = 'demiRecords.txt'\n",
    "output_file = 'output.csv'\n",
    "\n",
    "# Read the text file and parse the lines\n",
    "with open(input_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Preprocess the lines to handle 'null' as a string\n",
    "processed_lines = [line.strip().replace('null', \"'null'\") for line in lines]\n",
    "\n",
    "# Parse the lines into a list of dictionaries\n",
    "data = [ast.literal_eval(line) for line in processed_lines]\n",
    "\n",
    "# Define the CSV headers\n",
    "headers = ['FromServer', 'ToServer', 'time', 'action', 'processId']\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Data has been successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef503522-962d-4440-89cb-1e74026e26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "data_path = \"output.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d27b1d-6b56-4f22-8885-67af12f535ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list, struct\n",
    "\n",
    "# Group by processID and collect the sequence of actions\n",
    "processes_df = df.groupBy(\"processID\").agg(collect_list(struct(\"FromServer\", \"ToServer\", \"time\", \"action\")).alias(\"actions\"))\n",
    "\n",
    "# Convert actions to string for MinHash LSH\n",
    "def actions_to_string(actions):\n",
    "    return \"\".join([f\"{action['FromServer']}{action['ToServer']}\" for action in actions])\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "actions_to_string_udf = udf(actions_to_string, StringType())\n",
    "processes_df = processes_df.withColumn(\"actions_str\", actions_to_string_udf(col(\"actions\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cda6f26-141d-4d3b-b431-73b048182c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the median process lenght to aproximate\n",
    "\n",
    "# Calculate the length of actions_str column\n",
    "df_with_length = processes_df.withColumn(\"length\", length(\"actions_str\"))\n",
    "\n",
    "# Calculate the median length using window function and sorting\n",
    "windowSpec = Window.orderBy(\"length\")\n",
    "df_with_length = df_with_length.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "count_df = df_with_length.count()\n",
    "\n",
    "median_row = math.ceil(count_df / 2.0)\n",
    "\n",
    "median_length = df_with_length.filter(col(\"row_num\") == median_row).select(\"length\").first()\n",
    "cur_k = 5\n",
    "thresholds = [(10000, 9), (5000, 8), (1000, 7), (100, 6)]\n",
    "for threshold, value in thresholds:\n",
    "    if median_length[0] > threshold:\n",
    "        cur_k = value\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8079d96a-2044-4c38-a001-4c14c5c4af0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert actions string into shingles\n",
    "def get_shingles(row, k=5):\n",
    "    concatenated_str = ''.join(row)\n",
    "    shingles = [concatenated_str[i:i+k] for i in range(len(concatenated_str) - (k - 1))]\n",
    "    return shingles\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "get_shingles_udf = udf(lambda x: get_shingles(x,cur_k), ArrayType(StringType()))\n",
    "processes_df = processes_df.withColumn(\"shingles\", get_shingles_udf(col(\"actions_str\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a097b3c4-b0b0-40c8-8190-2f43a2d3511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time that it takes our model to find the pairs are  0.21000146865844727\n",
      "+-----------+-----------+-------------------+--------------------+--------------------+-----------------+\n",
      "|processID_A|processID_B|    JaccardDistance|           featuresA|           featuresB|JaccardSimilarity|\n",
      "+-----------+-----------+-------------------+--------------------+--------------------+-----------------+\n",
      "|          1|       1164|                0.0|(8739,[72,142,145...|(8739,[72,142,145...|              1.0|\n",
      "|          2|       5660|                0.0|(8739,[55,99,100,...|(8739,[55,99,100,...|              1.0|\n",
      "|         10|       2725|                0.0|(8739,[62,310,545...|(8739,[62,310,545...|              1.0|\n",
      "|         14|       2074|                0.0|(8739,[16,51,75,7...|(8739,[16,51,75,7...|              1.0|\n",
      "|         28|       7807|                0.0|(8739,[17,18,19,2...|(8739,[17,18,19,2...|              1.0|\n",
      "|         32|       1445|                0.4|(8739,[71,222,227...|(8739,[71,76,84,8...|              0.6|\n",
      "|         35|       7117| 0.3846153846153846|(8739,[52,181,182...|(8739,[52,130,131...|       0.61538464|\n",
      "|         40|        440|                0.0|(8739,[74,294,426...|(8739,[74,294,426...|              1.0|\n",
      "|         44|       5435|0.43333333333333335|(8739,[0,7,9,10,1...|(8739,[62,247,252...|       0.56666666|\n",
      "|         46|       8687|                0.0|(8739,[73,140,415...|(8739,[73,140,415...|              1.0|\n",
      "|         53|       1050|                0.0|(8739,[71,384,406...|(8739,[71,384,406...|              1.0|\n",
      "|         59|       2116|                0.0|(8739,[71,170,353...|(8739,[71,170,353...|              1.0|\n",
      "|         76|       1447|                0.0|(8739,[51,143,144...|(8739,[51,143,144...|              1.0|\n",
      "|         80|       8550| 0.4137931034482759|(8739,[17,18,19,2...|(8739,[4,5,6,8,17...|        0.5862069|\n",
      "|         81|       3517|                0.0|(8739,[37,77,106,...|(8739,[37,77,106,...|              1.0|\n",
      "|         85|       6643|                0.0|(8739,[51,249,253...|(8739,[51,249,253...|              1.0|\n",
      "|         91|       6015|                0.0|(8739,[74,239,241...|(8739,[74,239,241...|              1.0|\n",
      "|         99|       1759|                0.0|(8739,[72,142,145...|(8739,[72,142,145...|              1.0|\n",
      "|        100|       9224|                0.0|(8739,[62,321,420...|(8739,[62,321,420...|              1.0|\n",
      "|        108|       2383|                0.0|(8739,[72,258,294...|(8739,[72,258,294...|              1.0|\n",
      "+-----------+-----------+-------------------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, array_union, collect_list, explode, col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"shingles\", outputCol=\"features\",binary=True)\n",
    "cv_model = cv.fit(processes_df)\n",
    "vectorized_df = cv_model.transform(processes_df)\n",
    "start = time.time()\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10)\n",
    "mh_model = mh.fit(vectorized_df)\n",
    "\n",
    "hashed_df = mh_model.transform(vectorized_df)\n",
    "threshold = 0.5\n",
    "# Find similar candidate process IDs using MinHashLSH\n",
    "similarity_df = mh_model.approxSimilarityJoin(hashed_df, hashed_df, threshold, distCol=\"JaccardDistance\") \\\n",
    "    .select(col(\"datasetA.processID\").alias(\"processID_A\"),\n",
    "            col(\"datasetB.processID\").alias(\"processID_B\"),\n",
    "            col(\"JaccardDistance\"),col(\"datasetA.features\").alias(\"featuresA\"),col(\"datasetB.features\").alias(\"featuresB\"))\n",
    "\n",
    "# Filter out self-joins and duplicates\n",
    "similarity_df = similarity_df.filter(col(\"processID_A\") < col(\"processID_B\"))\n",
    "\n",
    "# Function to calculate Jaccard similarity\n",
    "def jaccard_similarity(vec1, vec2):\n",
    "    set1 = set(vec1.indices)\n",
    "    set2 = set(vec2.indices)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "    return float(len(intersection)) / len(union)\n",
    "\n",
    "# Register the function as a UDF\n",
    "from pyspark.sql.types import FloatType\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "\n",
    "# Calculate Jaccard similarity for each candidate pair\n",
    "similarity_df = similarity_df.withColumn(\"JaccardSimilarity\", jaccard_similarity_udf(col(\"featuresA\"), col(\"featuresB\")))\n",
    "# Filter pairs with Jaccard similarity above a threshold (e.g., 0.8)\n",
    "similarity_df_filtered = similarity_df.filter(col(\"JaccardSimilarity\") >= 0.9)\n",
    "end = time.time()\n",
    "print(\"the time that it takes our model to find the pairs are \",end-start)\n",
    "# Group by processID_A and collect similar processIDs\n",
    "grouped_df = similarity_df.groupBy(\"processID_A\").agg(collect_list(\"processID_B\").alias(\"similar_processIDs\"))\n",
    "\n",
    "# Convert processID_A to an array and concatenate with similar_processIDs\n",
    "grouped_df = grouped_df.withColumn(\"all_processIDs\", array_union(array(col(\"processID_A\")), col(\"similar_processIDs\")))\n",
    "\n",
    "# Explode the all_processIDs array to get a mapping of each process ID to its group\n",
    "exploded_df = grouped_df.select(explode(col(\"all_processIDs\")).alias(\"processID\"), col(\"processID_A\").alias(\"group_representative\"))\n",
    "similarity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5dcfb7e-4d37-4ddc-a840-b731a251c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge overlapping groups\n",
    "def merge_groups(group_list):\n",
    "    groups = []\n",
    "    for group in group_list:\n",
    "        merged = False\n",
    "        for existing_group in groups:\n",
    "            if any(item in group for item in existing_group):\n",
    "                existing_group.update(group)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            groups.append(set(group))\n",
    "    return [list(group) for group in groups]\n",
    "\n",
    "merge_groups_udf = udf(lambda x: merge_groups(x), ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "grouped_lists = exploded_df.groupBy(\"group_representative\") \\\n",
    "    .agg(collect_list(\"processID\").alias(\"group_list\")) \\\n",
    "    .agg(collect_list(\"group_list\").alias(\"group_lists\"))\n",
    "\n",
    "merged_groups = grouped_lists.withColumn(\"merged_groups\", merge_groups_udf(col(\"group_lists\"))) \\\n",
    "    .select(explode(col(\"merged_groups\")).alias(\"final_group\"))\n",
    "\n",
    "# Convert the final groups to a DataFrame\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "final_groups_df = merged_groups.select(concat_ws(\"_\", col(\"final_group\")).alias(\"Group\"), col(\"final_group\"))\n",
    "\n",
    "# Find the representative process for each final group\n",
    "final_groups_exploded = final_groups_df.withColumn(\"processID\", explode(col(\"final_group\")))\n",
    "\n",
    "# Join with the original DataFrame to keep only the representative process\n",
    "filtered_df = df.join(final_groups_exploded, on=\"processID\", how=\"inner\")\n",
    "\n",
    "# Select the smallest processID in each group as the representative\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "group_representative_df = final_groups_exploded.groupBy(\"Group\").agg(min(\"processID\").alias(\"representative_processID\"))\n",
    "\n",
    "# Join to get the full details of the representative processes\n",
    "representative_processes_df = group_representative_df.join(filtered_df, filtered_df[\"processID\"] == group_representative_df.representative_processID, \"inner\") \\\n",
    "    .select(\"processID\", \"FromServer\", \"ToServer\", \"time\", \"action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b5802c-cd0d-4286-b8fb-2eebdb8403b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "# Step 1: Remove Processes in Groups\n",
    "# Get the list of process IDs to remove\n",
    "processes_to_remove = final_groups_df.selectExpr(\"explode(final_group) as processID\").distinct()\n",
    "\n",
    "# Filter out rows where processID is in processes_to_remove\n",
    "df_without_groups = df.join(processes_to_remove, \"processID\", \"left_anti\")\n",
    "df_without_groups = df_without_groups.select(\"FromServer\", \"ToServer\", \"time\", \"action\",\"processID\")\n",
    "# Add a constant number to processID\n",
    "constant_number = df.agg({\"processID\": \"max\"}).first()[0]\n",
    "new_representative_processes_df = representative_processes_df.withColumn(\n",
    "    \"processID\",\n",
    "    expr(f\"processID + {constant_number}\")\n",
    ")\n",
    "\n",
    "# Show the final DataFrame\n",
    "new_representative_processes_df = new_representative_processes_df.select(\"FromServer\", \"ToServer\", \"time\", \"action\",\"processID\").orderBy(\"time\")\n",
    "\n",
    "# Combine original DataFrame and representatives DataFrame\n",
    "combined_df = df_without_groups.union(new_representative_processes_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fd6b4-fc29-4be1-bfe0-1fca39b52f08",
   "metadata": {},
   "source": [
    "# creating the txt files:\n",
    "## The desired files will be in the folder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fedf60b6-3e4f-4f7f-98ee-0a6a078a2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_one_txt(df, local_path_name,wanted_list):\n",
    "    correct_path = wanted_list + \"/part1Output.txt\"\n",
    "    formatted_df = df.withColumn(\n",
    "    \"formatted_line\",\n",
    "    concat(lit(\"<\"), df.FromServer, lit(\",\"),\n",
    "           df.ToServer, lit(\",\"),\n",
    "           df.time, lit(\",\"),\n",
    "           df.action, lit(\",\"),\n",
    "           df.processID, lit(\">\"))\n",
    ")\n",
    "    open(correct_path, \"w\")\n",
    "    formatted_df.select(\"formatted_line\").write.mode(\"overwrite\").text(output_path)\n",
    "    os.system(f'cat {local_path_name}/*.txt >> {correct_path}')\n",
    "    os.system(f'rm -r {local_path_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9f5fb-c15b-4912-9cbe-951cb816d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the output path\n",
    "output_path = \"./part1OUT1\"\n",
    "output_path1 = \"./output\"\n",
    "write_to_one_txt(combined_df,output_path,output_path1)\n",
    "# Write the DataFrame to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb262e7-90dd-43b2-a7c0-c429b39d384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe only with the processes that were grouped.\n",
    "df_with_groups = df.join(processes_to_remove, \"processID\", \"semi\")\n",
    "df_with_groups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ff333-dfeb-45fb-81fa-ae4feb700f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_final_groups_df = final_groups_df.select(\"Group\", explode(\"final_group\").alias(\"processID\"))\n",
    "joined_df = df_with_groups.join(exploded_final_groups_df, \"processID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023aabea-fe0c-472b-8408-b625b24103f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write groups to txt file\n",
    "def write_groups_to_txt(grouped_df, output_path):\n",
    "    with open(output_path, \"w\") as file:\n",
    "        for row in grouped_df.collect():\n",
    "            group_name = row[\"Group\"]\n",
    "            process_ids = row[\"processIDs\"]\n",
    "            formatted_rows = row[\"formatted_rows\"]\n",
    "            \n",
    "            # Ensure process_ids are unique and sorted\n",
    "            process_ids = sorted(set(process_ids))\n",
    "            \n",
    "            file.write(f\"Group: {{{', '.join(map(str, process_ids))}}}\\n\")\n",
    "            \n",
    "            for process_id in process_ids:\n",
    "                file.write(f\"{process_id}:\\n\")\n",
    "                \n",
    "                # Find all formatted rows for the current process ID\n",
    "                rows_for_process_id = [row for row in formatted_rows if row.endswith(f\",{process_id}>\")]\n",
    "                \n",
    "                if rows_for_process_id:\n",
    "                    for formatted_row in rows_for_process_id:\n",
    "                        file.write(f\"{formatted_row}\\n\")\n",
    "                else:\n",
    "                    file.write(\"<No corresponding formatted rows found>\\n\")\n",
    "                    \n",
    "            file.write(\"\\n\")  # Add empty line between groups for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a4565-628c-4fe5-9798-795445364f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format each row into the desired format\n",
    "formatted_df = joined_df.withColumn(\n",
    "    \"formatted_row\",\n",
    "    concat_ws(\"\", lit(\"<\"), col(\"FromServer\"), lit(\",\"), col(\"ToServer\"),\n",
    "              lit(\",\"), col(\"time\"), lit(\",\"), col(\"action\"), lit(\",\"), col(\"processID\"), lit(\">\"))\n",
    ")\n",
    "\n",
    "# Group by Group name and aggregate process IDs and formatted rows\n",
    "grouped_df = formatted_df.groupBy(\"Group\").agg(\n",
    "    collect_list(\"processID\").alias(\"processIDs\"),\n",
    "    collect_list(\"formatted_row\").alias(\"formatted_rows\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Output path\n",
    "output_path = \"./output/part1Observations.txt\"\n",
    "\n",
    "# Call function to write to text file\n",
    "write_groups_to_txt(grouped_df, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fff734-5f39-4196-80b1-b230b9cd6543",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1a508ce-9777-41aa-9a3e-15bc14dd9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "\n",
    "def evaluateData(original_df,df_similar_Minhash,threshold, process_id_col='processID'):\n",
    "    # start\n",
    "    start = time.time()\n",
    "    jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "    cross_joined_df = original_df.alias(\"df1\").join(original_df.alias(\"df2\")).select(col(\"df1.processID\").alias(\"processID_A\"),col(\"df1.features\").alias(\"processAFeatures\"),\n",
    "                                                                  col(\"df2.processID\").alias(\"processID_B\"),col(\"df2.features\").alias(\"processBFeatures\")).orderBy(col(\"processID_A\"))\n",
    "    filtered_df = cross_joined_df.filter(col(\"processID_A\") < col(\"processID_B\"))\n",
    "    # Calculate Jaccard similarity for each candidate pair\n",
    "    similarity_df = filtered_df.withColumn(\"JaccardSimilarity\", jaccard_similarity_udf(col(\"processAFeatures\"), col(\"processBFeatures\")))\n",
    "    \n",
    "    similarity_df = similarity_df.select(col(\"processID_A\"),col(\"processID_B\"),col(\"JaccardSimilarity\"))\n",
    "\n",
    "    t1 = similarity_df.filter(col(\"JaccardSimilarity\") >= threshold)\n",
    "    #stop\n",
    "    end = time.time()\n",
    "    print(\"the time it takes to calculate the original pairs are \",end - start)\n",
    "    print(\"This is the similar ds for Original\")\n",
    "    f1 = similarity_df.filter(col(\"JaccardSimilarity\") < threshold)\n",
    "    print(\"This is the disimilar ds for Original\")\n",
    "    df_similar_Minhash = df_similar_Minhash.select(col(\"processID_A\"),col(\"processID_B\"),col(\"JaccardSimilarity\"))\n",
    "    t3 = df_similar_Minhash.filter(col(\"JaccardSimilarity\") >= threshold)\n",
    "    f3 = df_similar_Minhash.filter(col(\"JaccardSimilarity\") < threshold)\n",
    "    TN = f1.subtract(t3)\n",
    "    TP = t3.intersect(t1)\n",
    "    FP = t3.intersect(f1)\n",
    "    FN = t1.subtract(t3)\n",
    "    return TN,TP,FP,FN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d22c462-1f6b-4e97-8723-8a775bc96cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time it takes to calculate the original pairs are  0.06589198112487793\n",
      "This is the similar ds for Original\n",
      "This is the disimilar ds for Original\n"
     ]
    }
   ],
   "source": [
    "TN,TP,FP,FN = evaluateData(vectorized_df,similarity_df,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961faf2-2d48-455a-babe-3f4162fe2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd29ac1-4e56-48a7-bc48-a43abd596f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce3c57f7-5073-4a1a-b9f7-a31a6f4094a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o471.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 4 times, most recent failure: Lost task 0.3 in stage 36.0 (TID 40) (192.168.1.81 executor 6): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.implRead(Socket.java:1108)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1095)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.implRead(Socket.java:1108)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1095)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m TN\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1405\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m \n\u001b[0;32m   1379\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;124;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit(num)\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m \n\u001b[0;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o471.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 4 times, most recent failure: Lost task 0.3 in stage 36.0 (TID 40) (192.168.1.81 executor 6): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.implRead(Socket.java:1108)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1095)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.implRead(Socket.java:1108)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1095)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "TN.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4b05b-adbf-4635-9831-0eef1f46e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f13ff-70d8-4ecc-beea-58eff5468752",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86959761-3c0d-4cfe-86ed-1e1dad3d8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, col, concat_ws, collect_list\n",
    "from pyspark.ml.feature import CountVectorizer, MinHashLSH\n",
    "from pyspark.sql.functions import array_union, explode,array\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, FloatType\n",
    "\n",
    "\n",
    "\n",
    "dataForPart2 = df\n",
    "dataForPart2.show()\n",
    "# Aggregate FromServer and ToServer into sets for each processId\n",
    "agg_df = dataForPart2.groupBy(\"processId\").agg(\n",
    "    collect_set(\"FromServer\").alias(\"servers_array\")\n",
    ")\n",
    "\n",
    "agg_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed413ee-b88b-4572-830b-ae9bdabb135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to convert server names to feature vectors\n",
    "cv = CountVectorizer(inputCol=\"servers_array\", outputCol=\"features\")\n",
    "cv_model = cv.fit(agg_df)\n",
    "cv_df = cv_model.transform(agg_df)\n",
    "\n",
    "cv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57b904-b6d0-440a-91c5-25c1effe4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Distinct Attributes (Vocabulary):\")\n",
    "for i, attr in enumerate(cv_model.vocabulary):\n",
    "    print(f\"{i}. {attr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8055dc-d0f0-4da5-b1b7-eff0fb9258f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show the transformed DataFrame with processId and features\n",
    "cv_df.select(\"processId\", \"features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08ac5b-ae05-4fea-a5ed-a39bb9d649a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply MinHash LSH\n",
    "numOftables = 10\n",
    "minhash = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=numOftables)\n",
    "model = minhash.fit(cv_df)\n",
    "transformed_df = model.transform(cv_df)\n",
    "\n",
    "\n",
    "threshold=0.5\n",
    "\n",
    "# Perform clustering based on MinHash LSH\n",
    "\n",
    "candidates = model.approxSimilarityJoin(transformed_df, transformed_df, threshold , distCol=\"JaccardDistance\") \\\n",
    "    .select(col(\"datasetA.processId\").alias(\"processIdA\"),\n",
    "            col(\"datasetB.processId\").alias(\"processIdB\"),\n",
    "            col(\"JaccardDistance\"),col(\"datasetA.features\").alias(\"featuresA\"),col(\"datasetB.features\").alias(\"featuresB\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f83a0d-a4ad-4067-a573-31dd7d65c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out self-joins and duplicates\n",
    "candidates = candidates.filter(col(\"processIdA\") < col(\"processIdB\"))\n",
    "# Function to calculate Jaccard similarity\n",
    "print(\"Candidates\")\n",
    "candidates.show()\n",
    "\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e31720-1014-4b2f-8168-02b18a621ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Jaccard similarity for each candidate pair\n",
    "similarity_df = candidates.withColumn(\"JaccardSimilarity\", jaccard_similarity_udf(col(\"featuresA\"), col(\"featuresB\")))\n",
    "similarity_df.show()\n",
    "\n",
    "\n",
    " # Filter pairs with Jaccard similarity above a threshold (e.g., 0.6)\n",
    "similarity_df = similarity_df.filter(col(\"JaccardSimilarity\") >= 0.3)\n",
    "\n",
    "similarity_df.show()\n",
    "\n",
    "merge_groups_udf = udf(lambda x: merge_groups(x), ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "grouped_lists = exploded_df.groupBy(\"group_representative\") \\\n",
    "    .agg(collect_list(\"processID\").alias(\"group_list\")) \\\n",
    "    .agg(collect_list(\"group_list\").alias(\"group_lists\"))\n",
    "\n",
    "merged_groups = grouped_lists.withColumn(\"merged_groups\", merge_groups_udf(col(\"group_lists\"))) \\\n",
    "    .select(explode(col(\"merged_groups\")).alias(\"final_group\"))\n",
    "\n",
    "# Convert the final groups to a DataFrame\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "final_groups_df = merged_groups.select(concat_ws(\"_\", col(\"final_group\")).alias(\"Group\"), col(\"final_group\"))\n",
    "\n",
    "final_groups_df.show()\n",
    "# Output path\n",
    "output_path = \"./output/part2Observations.txt\"\n",
    "\n",
    "processes_from_groups = final_groups_df.selectExpr(\"explode(final_group) as processID\").distinct()\n",
    "# creating a dataframe only with the processes that were grouped.\n",
    "df_with_groups = dataForPart2.join(processes_from_groups, \"processID\", \"semi\")\n",
    "df_with_groups.show()\n",
    "\n",
    "exploded_final_groups_df = final_groups_df.select(\"Group\", explode(\"final_group\").alias(\"processID\"))\n",
    "joined_df = df_with_groups.join(exploded_final_groups_df, \"processID\")\n",
    "# Call function to write to text file\n",
    "joined_df.show()\n",
    "# write_groups_to_txt(final_groups_df, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e51b4a-4b57-4ea8-ac33-95c487f434f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348d6ba3-e099-4ca4-8efc-6fec71f4bb81",
   "metadata": {},
   "source": [
    "# creating the txt files:\n",
    "## The desired files will be in the folder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055c8ff-d1d4-4024-8c2b-26dbbca0986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format each row into the desired format\n",
    "formatted_df = joined_df.withColumn(\n",
    "    \"formatted_row\",\n",
    "    concat_ws(\"\", lit(\"<\"), col(\"FromServer\"), lit(\",\"), col(\"ToServer\"),\n",
    "              lit(\",\"), col(\"time\"), lit(\",\"), col(\"action\"), lit(\",\"), col(\"processID\"), lit(\">\"))\n",
    ")\n",
    "\n",
    "# Group by Group name and aggregate process IDs and formatted rows\n",
    "grouped_df = formatted_df.groupBy(\"Group\").agg(\n",
    "    collect_list(\"processID\").alias(\"processIDs\"),\n",
    "    collect_list(\"formatted_row\").alias(\"formatted_rows\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Output path\n",
    "output_path = \"./output/part2Observations.txt\"\n",
    "\n",
    "# Call function to write to text file\n",
    "write_groups_to_txt(grouped_df, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215c3f8-208d-4cd6-985e-fa57346af8ce",
   "metadata": {},
   "source": [
    "# Evaluation Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b776b-80ee-4f83-827f-379effdbaf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function for part 2\n",
    "# Evaluation function\n",
    "\n",
    "def evaluateData(original_df,df_similar_Minhash,threshold, process_id_col='processID'):\n",
    "    jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "    cross_joined_df = original_df.alias(\"df1\").join(original_df.alias(\"df2\")).select(col(\"df1.processId\").alias(\"processIdA\"),col(\"df1.features\").alias(\"processAFeatures\"),\n",
    "                                                                  col(\"df2.processId\").alias(\"processIdB\"),col(\"df2.features\").alias(\"processBFeatures\")).orderBy(col(\"processIdA\"))\n",
    "    filtered_df = cross_joined_df.filter(col(\"processIdA\") < col(\"processIdB\"))\n",
    "    # Calculate Jaccard similarity for each candidate pair\n",
    "    similarity_df = filtered_df.withColumn(\"JaccardSimilarity\", jaccard_similarity_udf(col(\"processAFeatures\"), col(\"processBFeatures\")))\n",
    "    df_similar_Minhash = df_similar_Minhash.select(col(\"processIdA\"),col(\"processIdB\"),col(\"JaccardSimilarity\"))\n",
    "    similarity_df = similarity_df.select(col(\"processIdA\"),col(\"processIdB\"),col(\"JaccardSimilarity\"))\n",
    "    t1 = similarity_df.filter(col(\"JaccardSimilarity\") >= threshold)\n",
    "    f1 = similarity_df.filter(col(\"JaccardSimilarity\") < threshold)\n",
    "    t3 = df_similar_Minhash.filter(col(\"JaccardSimilarity\") >= threshold)\n",
    "    f3 = df_similar_Minhash.filter(col(\"JaccardSimilarity\") < threshold)\n",
    "    TN = f1.subtract(t3)\n",
    "    TP = t3.intersect(t1)\n",
    "    FP = t3.intersect(f1)\n",
    "    FN = t1.subtract(t3)\n",
    "    print(\"TN\" , TN.count(), \" TP \" , TP.count(), \" FP \" , FP.count(),  \" FN \" , FN.count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1a14d-a642-4be9-882d-054519714e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pairs with Jaccard similarity above a threshold (e.g., 0.6)\n",
    "evaluateData(cv_df,similarity_df,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d4486-1013-47ba-a5c9-dd7e268783eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
